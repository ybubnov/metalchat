Memory reduced inference
========================

.. warning::

   Work is in progress.

This guide shows how to run the inference of Llama model with the reduced memory utilization by
utilizing the :cpp:struct:`metalchat::filebuf_memory_allocator`.
